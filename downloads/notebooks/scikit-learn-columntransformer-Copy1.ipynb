{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, linewidth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the `ColumnTransformer`: applying different transformations to different features in a scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This work is supported by the Universit√© Paris-Saclay Center for Data Science*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- PELICAN_BEGIN_SUMMARY -->\n",
    "<p>\n",
    "Short summary: the <code>ColumnTransformer</code>, which allows to apply different transformers to different features, has landed in scikit-learn (the <a href=\"https://github.com/scikit-learn/scikit-learn/pull/9012\">PR</a> has been merged in master and this will be included in the upcoming release 0.20). \n",
    "</p>\n",
    "<!-- PELICAN_END_SUMMARY -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Real-world data often contains heterogenous data types. When processing the data before applying the final prediction model, we typically want to use different preprocessing steps and transformations for those different types of columns.  \n",
    "A simple example: we may want to scale the numerical features and one-hot encode the categorical features. \n",
    "\n",
    "Currently, scikit-learn does not provide a good solution to do this out of the box. You can do the preprocessing beforehand using eg pandas, or you can select subsets of columns and apply different transformers on them manually. But, that does not easily allow to put those preprocessing steps in a scikit-learn `Pipeline`, which can be important to avoid data leakage or to do a grid search over preprocessing parameters.\n",
    "\n",
    "There are third-party projects that try to address this. For example, the [`sklearn_pandas`](https://github.com/scikit-learn-contrib/sklearn-pandas) package has a `DataFrameMapper` that maps subsets of a DataFrame's coluns to a specific transformation. Many thanks to the authors of this library, as such \"contrib\" packages are essential in extending the functionality of scikit-learn, and to explore things that would take a long time in scikit-learn itself.  \n",
    "The `ColumnTransformer` aims to bring this functionality into the core scikit-learn library, with support for numpy arrays and sparse matrices, and good integration with the rest of scikit-learn.\n",
    "\n",
    "\n",
    "<!--\n",
    "\n",
    "When working with prediction problems, in many cases your dataset will contain categorical variables. These are non-numeric variables -- or if numeric, the values should not be interpreted as numeric values -- that typically consist of a limited number of unique values (the categories or the levels). \n",
    "On the other hand, most machine learning models require numeric input data. Therefore, categorical variables are *encoded*: they are converted to one or multiple numeric features.\n",
    "A well known example is one-hot or dummy encoding.\n",
    "\n",
    "\n",
    "Currently there is no good out-of-the-box solution in scikit-learn. There is the `OneHotEncoder` which provides one-hot encoding, but because it only works on integer columns and has a bit of an awkward API, it is rather limited in practice. \n",
    "Chris Mofitt recently wrote a nice guide on how to encode categorical variables in python ([see his blogpost](http://pbpython.com/categorical-encoding.html)). He shows different ways to solve this: by (mis)using the `LabelEncoder` (which is actually meant for the target variable, not for encoding features) or using pandas' `get_dummies`, etc.\n",
    "But none of these solutions are ideal for the simple cases or can readily be integrated in scikit-learn pipelines.\n",
    "\n",
    "The newly added `CategoricalEncoder` tries to solve this: provide a built-in way to encode your categorical variables with some common options (either a one-hot or dummy encoding or an ordinal encoding).\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the basic usage of the `ColumnTransformer`, let's load the titanic survival dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"https://raw.githubusercontent.com/amueller/scipy-2017-sklearn/master/notebooks/datasets/titanic3.csv\")\n",
    "# there is still a small problem with using the CategoricalEncoder and missing values,\n",
    "# so for now I am going to assume there are no missing values by dropping them\n",
    "titanic = titanic.dropna(subset=['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting some of the features and target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = titanic.survived.values\n",
    "features = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass     sex      age      fare embarked\n",
       "0       1  female  29.0000  211.3375        S\n",
       "1       1    male   0.9167  151.5500        S\n",
       "2       1  female   2.0000  151.5500        S\n",
       "3       1    male  30.0000  151.5500        S\n",
       "4       1  female  25.0000  151.5500        S"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains some categorical variables (\"pclass\", \"sex\" and \"embarked\"), and some numerical variables (\"age\" and \"fare\"). Note that the \"pclass\", although categorical, is already encoded as integers in the dataset. \n",
    "So let's use the `ColumnTransformer` to combine transformers for those two types of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, CategoricalEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (['age', 'fare'], StandardScaler()),\n",
    "    (['pclass', 'sex', 'embarked'], CategoricalEncoder())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above creates a simple preprocessing pipeline (that will be combined in a full prediction pipeline below) to scale the numerical features and one-hot encode the categorical features.  \n",
    "We can check this is indeed working as expected by transforming the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.057,  3.136,  1.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ],\n",
       "       [-2.012,  2.063,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  1.   ],\n",
       "       [-1.937,  2.063,  1.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ],\n",
       "       [ 0.013,  2.063,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  1.   ],\n",
       "       [-0.335,  2.063,  1.   ,  0.   ,  0.   ,  1.   ,  0.   ,  0.   ,  0.   ,  1.   ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.fit_transform(features).toarray()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, we specified the subsets of columns as lists. We can also use boolean masks (eg to make a selection of the columns based on the data types), integer positions and slices. Further, the `ColumnTransformer` allows you to specify wether to drop or pass through other columns that were not specified. See the [development docs](http://scikit-learn.org/dev/modules/generated/sklearn.compose.ColumnTransformer.html) for more details.\n",
    "\n",
    "**! This is new functionality in scikit-learn, so you are very welcome to try out the development version, experiment with it in your use cases, and provide feedback!** I am sure there are ways to further improve this functionality (the [PR](https://github.com/scikit-learn/scikit-learn/pull/9012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating in a full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    preprocess,\n",
    "    LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.804598\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "print(\"logistic regression score: %f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess2 = make_column_transformer(\n",
    "    (['age', 'fare'], StandardScaler()),\n",
    "    (['pclass', 'sex', 'embarked'], CategoricalEncoder(encoding='ordinal'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = make_pipeline(\n",
    "    preprocess2,\n",
    "    RandomForestClassifier(n_estimators=500, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest score: 0.793103\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "print(\"random forest score: %f\" % rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer': ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,\n",
       "          transformers=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True), ['age', 'fare']), ('categoricalencoder', CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "           encoding='ordinal', handle_unknown='error'), ['pclass', 'sex', 'embarked'])]),\n",
       " 'columntransformer__categoricalencoder': CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "           encoding='ordinal', handle_unknown='error'),\n",
       " 'columntransformer__categoricalencoder__categories': 'auto',\n",
       " 'columntransformer__categoricalencoder__dtype': numpy.float64,\n",
       " 'columntransformer__categoricalencoder__encoding': 'ordinal',\n",
       " 'columntransformer__categoricalencoder__handle_unknown': 'error',\n",
       " 'columntransformer__n_jobs': 1,\n",
       " 'columntransformer__remainder': 'passthrough',\n",
       " 'columntransformer__standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'columntransformer__standardscaler__copy': True,\n",
       " 'columntransformer__standardscaler__with_mean': True,\n",
       " 'columntransformer__standardscaler__with_std': True,\n",
       " 'columntransformer__transformer_weights': None,\n",
       " 'columntransformer__transformers': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   ['age', 'fare']),\n",
       "  ('categoricalencoder',\n",
       "   CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "             encoding='ordinal', handle_unknown='error'),\n",
       "   ['pclass', 'sex', 'embarked'])],\n",
       " 'memory': None,\n",
       " 'randomforestclassifier': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "             oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       " 'randomforestclassifier__bootstrap': True,\n",
       " 'randomforestclassifier__class_weight': None,\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__max_depth': None,\n",
       " 'randomforestclassifier__max_features': 'auto',\n",
       " 'randomforestclassifier__max_leaf_nodes': None,\n",
       " 'randomforestclassifier__min_impurity_decrease': 0.0,\n",
       " 'randomforestclassifier__min_impurity_split': None,\n",
       " 'randomforestclassifier__min_samples_leaf': 1,\n",
       " 'randomforestclassifier__min_samples_split': 2,\n",
       " 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestclassifier__n_estimators': 500,\n",
       " 'randomforestclassifier__n_jobs': 1,\n",
       " 'randomforestclassifier__oob_score': False,\n",
       " 'randomforestclassifier__random_state': 0,\n",
       " 'randomforestclassifier__verbose': 0,\n",
       " 'randomforestclassifier__warm_start': False,\n",
       " 'steps': [('columntransformer',\n",
       "   ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,\n",
       "            transformers=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True), ['age', 'fare']), ('categoricalencoder', CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "             encoding='ordinal', handle_unknown='error'), ['pclass', 'sex', 'embarked'])])),\n",
       "  ('randomforestclassifier',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "               oob_score=False, random_state=0, verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = features.dtypes == 'float'\n",
    "categorical_features = ~numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = make_column_transformer(\n",
    "    (numerical_features, StandardScaler()),\n",
    "    (categorical_features, CategoricalEncoder()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.804598\n"
     ]
    }
   ],
   "source": [
    "lr = make_pipeline(preprocess, LogisticRegression())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), ['age', 'fare']),\n",
    "    ('onehot', CategoricalEncoder(), ['pclass', 'sex', 'embarked'],),\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.804598\n"
     ]
    }
   ],
   "source": [
    "lr = make_pipeline(preprocess, LogisticRegression())\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"logistic regression score: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), ['age', 'fare']),\n",
    "    ('onehot', CategoricalEncoder(), ['pclass', 'sex', 'embarked'],),\n",
    "], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score: 0.808429\n"
     ]
    }
   ],
   "source": [
    "lr = make_pipeline(preprocess, LogisticRegression())\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"logistic regression score: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<782x12 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4368 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'columntransformer': ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,\n",
       "          transformers=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True), ['age', 'fare']), ('onehot', CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "           encoding='onehot', handle_unknown='error'), ['pclass', 'sex', 'embarked'])]),\n",
       " 'columntransformer__n_jobs': 1,\n",
       " 'columntransformer__onehot': CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "           encoding='onehot', handle_unknown='error'),\n",
       " 'columntransformer__onehot__categories': 'auto',\n",
       " 'columntransformer__onehot__dtype': numpy.float64,\n",
       " 'columntransformer__onehot__encoding': 'onehot',\n",
       " 'columntransformer__onehot__handle_unknown': 'error',\n",
       " 'columntransformer__remainder': 'passthrough',\n",
       " 'columntransformer__scaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'columntransformer__scaler__copy': True,\n",
       " 'columntransformer__scaler__with_mean': True,\n",
       " 'columntransformer__scaler__with_std': True,\n",
       " 'columntransformer__transformer_weights': None,\n",
       " 'columntransformer__transformers': [('scaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   ['age', 'fare']),\n",
       "  ('onehot',\n",
       "   CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "             encoding='onehot', handle_unknown='error'),\n",
       "   ['pclass', 'sex', 'embarked'])],\n",
       " 'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'logisticregression__C': 1.0,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'ovr',\n",
       " 'logisticregression__n_jobs': 1,\n",
       " 'logisticregression__penalty': 'l2',\n",
       " 'logisticregression__random_state': None,\n",
       " 'logisticregression__solver': 'liblinear',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False,\n",
       " 'memory': None,\n",
       " 'steps': [('columntransformer',\n",
       "   ColumnTransformer(n_jobs=1, remainder='passthrough', transformer_weights=None,\n",
       "            transformers=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True), ['age', 'fare']), ('onehot', CategoricalEncoder(categories='auto', dtype=<class 'numpy.float64'>,\n",
       "             encoding='onehot', handle_unknown='error'), ['pclass', 'sex', 'embarked'])])),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), ['age', 'fare']),\n",
    "    ('onehot', CategoricalEncoder(encoding='ordinal'), ['pclass', 'sex', 'embarked'],),\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest score: 0.793103\n"
     ]
    }
   ],
   "source": [
    "rf = make_pipeline(preprocess, RandomForestClassifier(n_estimators=500, random_state=0))\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"random forest score: %f\" % rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [development docs](http://scikit-learn.org/dev/modules/preprocessing.html#encoding-categorical-features) for more information.\n",
    "\n",
    "Having this conversion available as a sklearn transformer also makes it easier to put in a `Pipeline`. Although, this is at the moment not yet fully straightforward because we need to combine the output of this categorical encoder with the other numeric columns. Currently you can already use the [FeatureUnion](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html) or the [DataFrameMapper](https://github.com/scikit-learn-contrib/sklearn-pandas#transformation-mapping) from the sklearn-pandas project, but the future `ColumnTransformer` will provide a built-in way to make this much easier (this is another PR I am working on: https://github.com/scikit-learn/scikit-learn/pull/9012).\n",
    "\n",
    "\n",
    "**! This is brand new functionality in scikit-learn, so feedback is very welcome! (the [PR](https://github.com/scikit-learn/scikit-learn/pull/9151))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want more categorical encoders?\n",
    "\n",
    "The `CategoricalEncoder` only provides two ways to encode (one-hot or dummy, and ordinal encoding), but there are many more possible ways to convert your categorical variables into numeric features suited to feed into models. The [Category Encoders](http://contrib.scikit-learn.org/categorical-encoding/) is a scikit-learn-contrib package that provides a whole suite of scikit-learn compatible transformers for different types of categorical encodings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (dev)",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
